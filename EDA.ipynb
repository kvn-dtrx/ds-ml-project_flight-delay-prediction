{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS ML Project ‚ü° Flight Delay Prediction Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "**TODO** Write this paragraph\n",
    "\n",
    "This notebook represents our analysis of the [flight delay dataset for Tunisair](https://zindi.africa/competitions/flight-delay-prediction-challenge) from [Zindi](https://zindi.africa) \n",
    "\n",
    "At last: \"our\" refers to ...\n",
    "- [greseberisha](https://github.com/greseberisha)\n",
    "- [MoSeBaur](https://github.com/MoSeBaur)\n",
    "- [kvn-dtrx](https://github.com/kvn-dtrx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science\n",
    "import pandas as pd\n",
    "\n",
    "# Scientific Computation\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-Learn Tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Power predictive score\n",
    "import ppscore as pps\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs\n",
    "\n",
    "Next, let us specify all required configurations to have them in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level of the warnings module\n",
    "WARNINGS_LEVEL = \"ignore\"\n",
    "warnings.filterwarnings(WARNINGS_LEVEL)\n",
    "\n",
    "# Path to train data\n",
    "PATH_DATA_TRAIN = \"./data/train.csv\"\n",
    "\n",
    "# Random seed\n",
    "RSEED = 42\n",
    "\n",
    "# Resolution when storing plots in files\n",
    "DPI = 600\n",
    "\n",
    "# Matplotlib style\n",
    "PLT_STYLE = \"seaborn\"\n",
    "try:\n",
    "    plt.style.use(PLT_STYLE)\n",
    "except:\n",
    "    warnings.warn(f\"Could not load matplotlib style {PLT_STYLE}\", UserWarning)\n",
    "\n",
    "# Whether to run long computations\n",
    "RUN_LONG_COMPUTATIONS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Confrontation with the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data from file into a pandas data frame and create a copy that will incorporate our manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv(PATH_DATA_TRAIN)\n",
    "\n",
    "df = df_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual initial inspection commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())\n",
    "print(df.dtypes)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning of column names (according to <https://zindi.africa/competitions/flight-delay-prediction-challenge/data>):\n",
    "\n",
    "Present in the data:\n",
    "\n",
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| ID | Unique identifier for the flight |\n",
    "| DATOP | Date of flight |\n",
    "| FLTID | Flight number |\n",
    "| DEPSTN | Departure point (station/airport) |\n",
    "| ARRSTN | Arrival point (station/airport) |\n",
    "| STD | Scheduled Time of Departure |\n",
    "| STA | Scheduled Time of Arrival |\n",
    "| STATUS | Flight status (e.g., delayed, canceled) |\n",
    "| AC | Aircraft code |\n",
    "| target | Flight delay (in minutes) |\n",
    "\n",
    "\n",
    "Not present in the data (although claimed on the referenced web page):\n",
    "\n",
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| ETD | Expected Time departure |\n",
    "| ETA | Expected Time arrival |\n",
    "| ATD | Actual Time of Departure |\n",
    "| ATA | Actual Time of arrival |\n",
    "| DELAY1 | Delay code 1 |\n",
    "| DUR1 | Delay time 1 |\n",
    "| DELAY2 | Delay code 2 |\n",
    "| DUR2 | Delay time 2 |\n",
    "| DELAY3 | Delay code 3 |\n",
    "| DUR3 | Delay time 3 |\n",
    "| DELAY4 | Delay code 4 |\n",
    "| DUR4 | Delay time 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorry for \"statuses\" ...\n",
    "statuses = df[\"STATUS\"].unique()\n",
    "\n",
    "print(\"All Statuses:\")\n",
    "for status in statuses:\n",
    "    print(f\"  Number of entries of {status}: {df[df['STATUS'] == status].shape[0]}\")\n",
    "    print(f\"  Mean: {df[df['STATUS'] == status]['target'].mean()}\")\n",
    "    print(f\"  Median: {df[df['STATUS'] == status]['target'].median()}\")\n",
    "\n",
    "for status in statuses:\n",
    "    df[df[\"STATUS\"] == status][\"target\"].hist(\n",
    "        bins=50,\n",
    "        log=False,\n",
    "    )\n",
    "    plt.title(status)\n",
    "    plt.xlabel(\"Delay\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.savefig(f\"./img/delay-to-sum-flight-on-status-eq-{status}_hist.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Code | Name              | Description                                                                 |\n",
    "|------|-------------------|-----------------------------------------------------------------------------|\n",
    "| ATA  | Actual Time Arrival| Flights that successfully landed at their destination.                     |\n",
    "| DEP  | Departed          | Flights that departed but may not have completed their journey.             |\n",
    "| RTR  | Returned          | Flights that took off but returned to the departure airport due to issues.  |\n",
    "| SCH  | Scheduled         | Flights listed in the schedule, no delay data applicable.                   |\n",
    "| DEL  | Canceled          | Flights that were canceled, treated as permanent delays.                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of DEP remains a bit obscure ... in a first approximation, we drop it. Further, it is hard to measure the delay of a DEL flight (a possibility for regular flights would be to take the duration between the DEL flight and the next flight that indeed arrives plus the delay of that flight). But we also decide us simply for dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"STATUS\"].isin([\"DEP\", \"DEL\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Columns\n",
    "\n",
    "We introduce columns reducing the airports of departure and destination to its country, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv(\"airports.csv\")\n",
    "airports = airports[[\"iata_code\", \"iso_country\"]]\n",
    "airports = airports.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Make this cell idempotent\n",
    "\n",
    "df = df.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    left_on=\"DEPSTN\",\n",
    "    right_on=\"iata_code\",\n",
    "    how=\"left\",\n",
    ")\n",
    "df.drop(columns=\"iata_code\", inplace=True)\n",
    "df.rename(columns={\"iso_country\": \"country_dep\"}, inplace=True)\n",
    "\n",
    "df = df.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    left_on=\"ARRSTN\",\n",
    "    right_on=\"iata_code\",\n",
    "    how=\"left\",\n",
    ")\n",
    "df.drop(columns=\"iata_code\", inplace=True)\n",
    "df.rename(columns={\"iso_country\": \"country_arr\"}, inplace=True)\n",
    "\n",
    "df[\"country_arr\"].shape\n",
    "\n",
    "df.loc[df[\"DEPSTN\"] == \"SXF\", \"country_dep\"] = \"DE\"\n",
    "df.loc[df[\"ARRSTN\"] == \"SXF\", \"country_arr\"] = \"DE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For converting the iso codes to continent codes, we use the functionality provided by the module `pycountry_convert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "\n",
    "def iso_to_continent(iso):\n",
    "    try:\n",
    "        continent_code = pc.country_alpha2_to_continent_code(iso)\n",
    "        return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"continent_dep\"] = df[\"country_dep\"].apply(iso_to_continent)\n",
    "df[\"continent_arr\"] = df[\"country_arr\"].apply(iso_to_continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dating issues\n",
    "\n",
    "The data set contains several columns with date semantics. Let us convert them to the appropriate dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DATOP\"] = pd.to_datetime(df[\"DATOP\"], format=\"%Y-%m-%d\")\n",
    "df[\"STD\"] = pd.to_datetime(df[\"STD\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"STA\"] = pd.to_datetime(df[\"STA\"], format=\"%Y-%m-%d %H.%M.%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can introduce a bunch of further useful date and time related columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DATOP_year\"] = df[\"DATOP\"].dt.year\n",
    "df[\"DATOP_month\"] = df[\"DATOP\"].dt.month\n",
    "df[\"DATOP_day\"] = df[\"DATOP\"].dt.dayofweek + 1\n",
    "\n",
    "def map_hour_to_period(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"day\"\n",
    "    elif 18 <= hour < 24:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "\n",
    "df[\"STD_hour\"] = df[\"STD\"].dt.hour\n",
    "df[\"STD_period\"] = df[\"STD_hour\"].apply(map_hour_to_period)\n",
    "\n",
    "df[\"flight_time\"] = (df[\"STA\"] - df[\"STD\"]).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which years are actually present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATOP_years = df[\"DATOP_year\"].unique()\n",
    "DATOP_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data are from the years 2016, 2017, 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=len(DATOP_years), figsize=(16, 5), sharey=True)\n",
    "\n",
    "for idx, year in enumerate(DATOP_years):\n",
    "    # Filter the DataFrame for the specific year\n",
    "    df_year = df[df[\"DATOP_year\"] == year]\n",
    "    \n",
    "    # Plot the histogram on the respective subplot\n",
    "    axes[idx].hist(df_year[\"DATOP_month\"], bins=range(1, 14), alpha=0.8, color=\"blue\")\n",
    "    axes[idx].set_title(f\"Flight Distribution for {year}\")\n",
    "    axes[idx].set_xlabel(\"Month\")\n",
    "    axes[idx].set_xticks(range(1, 13))  # Set x-axis ticks for months\n",
    "    axes[idx].set_ylabel(\"Number of Flights\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the combined plot\n",
    "plt.savefig(\"./img/month-to-sum-flight-by-year_hist.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each year, we find a suspicious months in which the sum of flights is significantly less than in the others. Looking at the provided test data set one sees that the majority of flights for the affected months can be found there (sic!) ... We drop these months completely: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df[\"DATOP_month\"] == 5) & (df[\"DATOP_year\"] == 2016))]\n",
    "df = df[~((df[\"DATOP_month\"] == 2) & (df[\"DATOP_year\"] == 2017))]\n",
    "df = df[~((df[\"DATOP_month\"] == 9) & (df[\"DATOP_year\"] == 2018))]\n",
    "\n",
    "\n",
    "df = df[df[\"DEPSTN\"] != df[\"ARRSTN\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df[\"flight_time\"], df[\"target\"], color=\"blue\")\n",
    "plt.xlabel(\"Flight Time\")\n",
    "plt.ylabel(\"Delay\")\n",
    "plt.xlim(1,3000)\n",
    "plt.ylim(1,3000)\n",
    "plt.title(\"Scatter Plot of X Column vs Y Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in DATOP_years:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    df_year = df[df[\"DATOP_year\"] == year]\n",
    "    df_year.groupby(\"DATOP_month\")[\"target\"].mean().plot(\n",
    "        kind=\"line\",\n",
    "        title=f\"Monthly Average of Delay for {year}\",\n",
    "        xlabel=\"Month\",\n",
    "        ylabel=\"Average of Delay\",\n",
    "    )\n",
    "    plt.savefig(f\"./img/month-to-avg-delay-on-status-eq-{status}_line.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Look at the Brushed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatterplot gives a feeling for single and pairwise distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LONG_COMPUTATIONS:\n",
    "    sns.pairplot(df)\n",
    "\n",
    "    plt.savefig(\"./img/each-vs-each-wrt-distribution_scatterplot.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the correlation matrix is never a bad idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    # cmap=\"Reds\",\n",
    "    fmt=\".2f\",\n",
    ")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "plt.savefig(\"./img/each-vs-each-wrt-correlation_heatmap.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as we have so many categorical features, it makes sense to compute the power predictive score (pps) matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LONG_COMPUTATIONS:\n",
    "\n",
    "    cols = [\n",
    "        col for col in df.columns\n",
    "        if not col.startswith(\"DATOP_\") and col not in [\"ID\"]\n",
    "    ]\n",
    "    \n",
    "    df_tmp = df[cols]\n",
    "\n",
    "    pp_scores = pps.matrix(df_tmp)[[\"x\", \"y\", \"ppscore\"]].pivot(\n",
    "        columns=\"x\", index=\"y\", values=\"ppscore\"\n",
    "    )\n",
    "\n",
    "    pp_scores = pp_scores.round(2)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    sns.heatmap(\n",
    "        pp_scores,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        # cmap=\"Reds\",\n",
    "        linewidths=0.5,\n",
    "        annot=True,\n",
    "    )\n",
    "\n",
    "    plt.savefig(\"./img/each-vs-each-wrt-pp-score_heatmap.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Considerations About Flight Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Verteilung der Versp√§tungen unter Abluegen und Ankuenften\n",
    "# Durchschnittliche Versp√§tung pro Abflughafen\n",
    "dep_delay = df.groupby(\"DEPSTN\")[\"target\"].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dep_delay.plot(kind=\"bar\", color=\"skyblue\")\n",
    "plt.title(\"Durchschnittliche Versp√§tung pro Abflughafen\")\n",
    "plt.xlabel(\"Abflughafen\")\n",
    "plt.ylabel(\"Durchschnittliche Versp√§tung (Minuten)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./img/dep-to-avg-delay_hist.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchschnittliche Versp√§tung pro Ankunftsflughafen\n",
    "arr_delay = df.groupby(\"ARRSTN\")[\"target\"].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "arr_delay.plot(kind=\"bar\", color=\"salmon\")\n",
    "plt.title(\"Durchschnittliche Versp√§tung pro Ankunftsflughafen\")\n",
    "plt.xlabel(\"Ankunftsflughafen\")\n",
    "plt.ylabel(\"Durchschnittliche Versp√§tung (Minuten)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./img/dest-to-avg-delay_hist.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x=\"DEPSTN\", y=\"target\", data=df)\n",
    "plt.title(\"Verteilung der Versp√§tungen pro Abflughafen\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"./img/airport-to-delay_boxplot.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot-Tabelle f√ºr Heatmap\n",
    "route_delay = df.pivot_table(\n",
    "    index=\"ARRSTN\", columns=\"DEPSTN\", values=\"target\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(route_delay, cmap=\"Reds\", linewidths=0.5, annot=False)\n",
    "plt.title(\"Durchschnittliche Versp√§tung zwischen Abflug- und Ankunftsflugh√§fen\")\n",
    "plt.xlabel(\"Ankunftsflughafen\")\n",
    "plt.ylabel(\"Abflughafen\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./img/dep-vs-dest-wrt-avg-delay_heatmap.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"DEPSTN\"] == \"TUN\"][\"target\"].hist(bins=30)\n",
    "plt.title(\"Versp√§tungsverteilung ‚Äì Abflughafen TUN\")\n",
    "plt.xlabel(\"Versp√§tung in Minuten\")\n",
    "plt.ylabel(\"Anzahl Fl√ºge\")\n",
    "plt.savefig(f\"./img/airport-to-delay-on-dest-eq-TUN_boxplot.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot_table(\n",
    "    index=\"DEPSTN\", columns=\"ARRSTN\", values=\"target\", aggfunc=\"mean\"\n",
    ")\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot, annot=False, cmap=\"Reds\")\n",
    "plt.title(\"Durchschnittliche Versp√§tung je Flugroute (in Minuten)\")\n",
    "plt.xlabel(\"Ankunftsflughafen\")\n",
    "plt.ylabel(\"Abflughafen\")\n",
    "plt.savefig(f\"./img/dep-vs-dest-wrt-delay_heatmap.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dep = df[\"DEPSTN\"].value_counts().head(10)\n",
    "avg_delay_dep = df.groupby(\"DEPSTN\")[\"target\"].mean().loc[top_dep.index]\n",
    "\n",
    "summary = pd.DataFrame({\"Fluganzahl\": top_dep, \"√ò Versp√§tung (Min.)\": avg_delay_dep})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].hist(\n",
    "    bins=100, \n",
    "    log=False,\n",
    ")\n",
    "plt.xlabel(\"Delay\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(1, 1000)\n",
    "\n",
    "plt.savefig(f\"./img/delay-to-sum-flight_hist.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (a.k.a. A Feeble Try), Version I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: The flight delay can be predicted from the Aircraft Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=[\"DATOP_day\"], prefix=\"AC\")\n",
    "\n",
    "y = df.target\n",
    "X = df_encoded.drop(\"target\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RSEED\n",
    ")\n",
    "\n",
    "cols = [col for col in df_encoded.columns if col.startswith(\"AC_\")]\n",
    "\n",
    "X_0 = X_train[cols]\n",
    "y_0 = y_train\n",
    "X_1 = X_test[cols]\n",
    "y_1 = y_test\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_0, y_0)\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysing the prediction errors we plot for the test set the actual values against the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_1)\n",
    "\n",
    "mse = mean_squared_error(y_1, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_1, y_pred)\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "t = 1000\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.plot([0, t], [0, t], color='red', linestyle='--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.xlim(0, 3000)\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.savefig(f\"./img/actual-vs-predicted_dist.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Model Using CatBoost\n",
    "\n",
    "As among the features there are many categorical ones, we choose a method that claims to be suited for such situations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "target_col = \"target\"\n",
    "feature_cols = [\n",
    "    \"STATUS\",\n",
    "    \"FLTID\",\n",
    "    \"AC\",\n",
    "    \"flight_time\",\n",
    "    \"DEPSTN\", \n",
    "    \"ARRSTN\",\n",
    "    \"DATOP_year\", \n",
    "    \"DATOP_month\", \n",
    "    \"DATOP_day\",\n",
    "    \"STD\",\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"STATUS\",\n",
    "    \"FLTID\",\n",
    "    \"AC\",\n",
    "    # \"flight_time\",\n",
    "    \"DEPSTN\", \n",
    "    \"ARRSTN\",\n",
    "    # \"DATOP_year\", \n",
    "    # \"DATOP_month\", \n",
    "    # \"DATOP_day\",\n",
    "    # \"STD\",\n",
    "]\n",
    "\n",
    "y = df[target_col]\n",
    "X = df[feature_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RSEED\n",
    ")\n",
    "\n",
    "model = CatBoostRegressor(verbose=0)\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    # Number of boosting rounds (trees). \n",
    "    # Higher values allow the model to capture complex patterns but may increase risk of overfitting.\n",
    "    \"iterations\": [300, 500],\n",
    "\n",
    "    # Learning rate controls the step size in gradient descent.\n",
    "    # Smaller values slow down learning but reduce the risk of overshooting the optimal solution.\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    # \"learning_rate\": [0.05, 0.1],\n",
    "\n",
    "    # Maximum depth of each decision tree.\n",
    "    # Larger values allow the model to capture more intricate feature interactions but may increase overfitting.\n",
    "    \"depth\": [8, 10, 12],\n",
    "\n",
    "    # Specifies the categorical columns in the dataset.\n",
    "    # CatBoost handles these features differently, such as using target statistics or one-hot encoding.\n",
    "    \"cat_features\": [cat_cols],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5, \n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R_2: {r2}\")\n",
    "\n",
    "t = 1000\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.plot([0, t], [0, t], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.xlim(0, 3000)\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.savefig(f\"./img/actual-vs-predicted_dist.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Two-Level Estimator\n",
    "\n",
    "A try to combine a classifier and a regressor in the following way:\n",
    "- If the classifier predicts that the delay is below a certain threshold return zero.\n",
    "- Otherwise, use the prediction of the regressor that is trained on a curated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "THRESH = 600\n",
    "\n",
    "THRESHS = [75, 100, 125, 150, 600]\n",
    "\n",
    "for THRESH in THRESHS:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=0.2, \n",
    "        random_state=RSEED,\n",
    "        stratify = (y < THRESH).astype(int)\n",
    "    )\n",
    "\n",
    "    p_train = (y_train < THRESH).astype(int)\n",
    "\n",
    "    classifier = CatBoostClassifier(\n",
    "        verbose=0,\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=1,\n",
    "        cat_features=cat_cols,\n",
    "    )\n",
    "\n",
    "    classifier.fit(X_train, p_train)\n",
    "\n",
    "    # Caveat: This is a prediction on the training set!\n",
    "    p_predict_0_ = classifier.predict(X_train)\n",
    "    p_predict_ = p_predict_0_.astype(bool)\n",
    "\n",
    "    X_0 = X_train[p_predict_]\n",
    "    y_0 = y_train[p_predict_]\n",
    "\n",
    "    regressor = CatBoostRegressor(\n",
    "        verbose=0,\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=4,\n",
    "        cat_features=cat_cols,\n",
    "    )\n",
    "\n",
    "    regressor.fit(X_0, y_0)\n",
    "\n",
    "    p_pred = classifier.predict(X_test)\n",
    "    y_pred = p_pred * regressor.predict(X_test)\n",
    "\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"THRESH: {THRESH}\")\n",
    "    print(f\"pmean: {np.mean(p_predict_0_)}\")\n",
    "    print(f\"pmean: {np.mean(p_pred)}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R_2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoSe Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(\n",
    "    df, columns=[\"DATOP_day\"], prefix=\"day\", drop_first=True, dtype=int\n",
    ")\n",
    "df2 = pd.get_dummies(\n",
    "    df2, columns=[\"DATOP_year\"], prefix=\"yr\", drop_first=True, dtype=int\n",
    ")\n",
    "df2 = pd.get_dummies(\n",
    "    df2, columns=[\"DATOP_month\"], prefix=\"mon\", drop_first=True, dtype=int\n",
    ")\n",
    "df2 = pd.get_dummies(df2, columns=[\"DEPSTN\"], prefix=\"dep\", drop_first=True, dtype=int)\n",
    "df2 = pd.get_dummies(df2, columns=[\"ARRSTN\"], prefix=\"arr\", drop_first=True, dtype=int)\n",
    "df2 = pd.get_dummies(df2, columns=[\"AC\"], prefix=\"ac\", drop_first=True, dtype=int)\n",
    "df2 = pd.get_dummies(\n",
    "    df2, columns=[\"STD_period\"], prefix=\"std\", drop_first=True, dtype=int\n",
    ")\n",
    "df2 = pd.get_dummies(\n",
    "    df2, columns=[\"STA_period\"], prefix=\"sta\", drop_first=True, dtype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2.target\n",
    "X = df2.drop(\"target\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RSEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"day_\", \"yr_\", \"mt_\", \"ac_\", \"dep_\", \"arr_\", \"std_\", \"sta_\"]\n",
    "\n",
    "# Collect columns that match those prefixes\n",
    "feature_cols = [\n",
    "    col for col in df2.columns if any(col.startswith(p) for p in prefixes)\n",
    "] + [\"flight_time\"]\n",
    "\n",
    "x0 = X_train[feature_cols]\n",
    "x1 = X_test[feature_cols]\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "# model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "model.fit(x0, y_train)\n",
    "y_pred_test = model.predict(x1)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "print(r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100],\n",
    "    \"max_depth\": [20, 30],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=\"neg_mean_squared_error\",  # or use 'r2' if you prefer\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x0, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test = best_model.predict(x1)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [10, 30],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(x0, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test = best_model.predict(x1)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
