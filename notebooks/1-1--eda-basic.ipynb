{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Exploratory Analysis\n",
    "\n",
    "## Setup\n",
    "\n",
    "For the purposes of our analysis, the following modules shall be required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extension of Python package pycountry providing conversion functions\n",
    "import pycountry_convert as pc\n",
    "\n",
    "# Python implementation of the Predictive Power Score (PPS)\n",
    "import ppscore as pps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipynb_utils import CFG, plt_savefig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load the data frame containing the flight delay durations, which must be obtained manually (cf. the preceding notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG[\"TRAIN_DATA_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Inspection\n",
    "\n",
    "As a preliminary inspection, let us invoke the info and sample methods of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains no missing values. The meaning of the column names may be found at <https://zindi.africa/competitions/flight-delay-prediction-challenge/data>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| ID | Unique identifier for the flight |\n",
    "| DATOP | Date of flight |\n",
    "| FLTID | Flight number |\n",
    "| DEPSTN | Departure point (station/airport) |\n",
    "| ARRSTN | Arrival point (station/airport) |\n",
    "| STD | Scheduled Time of Departure |\n",
    "| STA | Scheduled Time of Arrival |\n",
    "| STATUS | Flight status (e.g., delayed, canceled) |\n",
    "| AC | Aircraft code |\n",
    "| target | Flight delay (in minutes) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of our dataset with the information provided on the aforementioned webpage reveals that additional features are documented there which are absent from our data frame — specifically, the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| ETD | Expected Time departure |\n",
    "| ETA | Expected Time arrival |\n",
    "| ATD | Actual Time of Departure |\n",
    "| ATA | Actual Time of arrival |\n",
    "| DELAY1 | Delay code 1 |\n",
    "| DUR1 | Delay time 1 |\n",
    "| DELAY2 | Delay code 2 |\n",
    "| DUR2 | Delay time 2 |\n",
    "| DELAY3 | Delay code 3 |\n",
    "| DUR3 | Delay time 3 |\n",
    "| DELAY4 | Delay code 4 |\n",
    "| DUR4 | Delay time 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Inspection\n",
    "\n",
    "Let us now examine certain features of the data frame in greater detail, en passant cleansing the data.\n",
    "\n",
    "### Status Column\n",
    "\n",
    "We begin by examining the STATUS feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The etymologically correct plural of the Latin word *status* [ˈstaː.tus]\n",
    "# is *status* [ˈstaː.tuːs]!\n",
    "\n",
    "statuses = df[\"STATUS\"].unique()\n",
    "\n",
    "print(\"All Statuses:\")\n",
    "for status in statuses:\n",
    "    print(f\"  {status}\")\n",
    "    print(f\"    Number of entries : {df[df['STATUS'] == status].shape[0]}\")\n",
    "    print(f\"    Mean              : {df[df['STATUS'] == status]['target'].mean()}\")\n",
    "    print(f\"    Median            : {df[df['STATUS'] == status]['target'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table elucidates the status codes that arise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code | Name | Description |\n",
    "| --- | --- | --- |\n",
    "| ATA | Actual Time Arrival| Flights that successfully landed at their destination |\n",
    "| DEP  | Departed | Flights that departed but may not have completed their journey |\n",
    "| RTR  | Returned | Flights that took off but returned to the departure airport due to issues |\n",
    "| SCH  | Scheduled | Flights listed in the schedule, no delay data applicable |\n",
    "| DEL  | Cancelled | Flights that were canceled, treated as permanent delays |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise the distribution of delay durations conditioned on STATUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, status in enumerate(statuses):\n",
    "    ax = axes[idx]\n",
    "    df[df[\"STATUS\"] == status][\"target\"].hist(bins=50, log=False, ax=ax)\n",
    "    ax.set_title(status)\n",
    "    ax.set_xlabel(\"Delay\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt_savefig(\"delay-to-sum-flight-histograms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of DEP (\"Flights that departed but may not have completed their journey\") remains somewhat obscure. As only a minority of flights are labelled as DEP, we have chosen to omit the corresponding data points.\n",
    "\n",
    "Furthermore, measuring the delay of a DEL (cancelled) flight proves difficult. One might consider, for regular flights, calculating the interval between the cancelled flight and the subsequent flight that indeed arrives, adding the delay of that latter flight. We have also chosen to delete rows the corresponding rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"STATUS\"].isin([\"DEP\", \"DEL\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Columns\n",
    "\n",
    "We introduce columns that reduce the airports of departure and destination to their respective country.  For this purpose, we require the airports.csv file (located at CFG[\"AIRPORTS_DATA_PATH\"]) downloaded in the preceding notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = (\n",
    "    pd.read_csv(CFG[\"AIRPORTS_DATA_PATH\"])\n",
    "    .loc[:, [\"iata_code\", \"iso_country\"]]\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We incorporate the relevant airport information into our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A data frame to make the next cell idempotent.\n",
    "df_bkp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges departure.\n",
    "dep_countries = airports.loc[:, [\"iata_code\", \"iso_country\"]].rename(\n",
    "    columns={\"iata_code\": \"DEPSTN\", \"iso_country\": \"country_dep\"}\n",
    ")\n",
    "df_tmp = df_bkp.merge(dep_countries, on=\"DEPSTN\", how=\"left\")\n",
    "\n",
    "# Merges arrival.\n",
    "arr_countries = airports.loc[:, [\"iata_code\", \"iso_country\"]].rename(\n",
    "    columns={\"iata_code\": \"ARRSTN\", \"iso_country\": \"country_arr\"}\n",
    ")\n",
    "df_tmp = df_tmp.merge(arr_countries, on=\"ARRSTN\", how=\"left\")\n",
    "\n",
    "df = df_tmp\n",
    "\n",
    "# ENIGMA: Why was this correction necessary?\n",
    "df.loc[df[\"DEPSTN\"] == \"SXF\", \"country_dep\"] = \"DE\"\n",
    "df.loc[df[\"ARRSTN\"] == \"SXF\", \"country_arr\"] = \"DE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the ISO codes to continent codes, we employ the functionality provided by the module `pycountry_convert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_to_continent(iso: str) -> None | str:\n",
    "    try:\n",
    "        continent_code = pc.country_alpha2_to_continent_code(iso)\n",
    "        return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"continent_dep\"] = df[\"country_dep\"].apply(iso_to_continent)\n",
    "df[\"continent_arr\"] = df[\"country_arr\"].apply(iso_to_continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let us remove all flights from the data frame for which the departure and arrival airports coincide. Very likely, these represent merely service flights and no genuine flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df[\"DEPSTN\"] != df[\"ARRSTN\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-related Columns\n",
    "\n",
    "The dataset contains several columns bearing date semantics. Let us convert them to the appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"DATOP\"] = pd.to_datetime(df[\"DATOP\"], format=\"%Y-%m-%d\")\n",
    "df.loc[:, \"STD\"] = pd.to_datetime(df[\"STD\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df.loc[:, \"STA\"] = pd.to_datetime(df[\"STA\"], format=\"%Y-%m-%d %H.%M.%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now introduce several additional useful features relating to dates and times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DATOP_year\"] = df[\"DATOP\"].dt.year\n",
    "df[\"DATOP_month\"] = df[\"DATOP\"].dt.month\n",
    "df[\"DATOP_day\"] = df[\"DATOP\"].dt.dayofweek + 1\n",
    "\n",
    "\n",
    "def map_hour_to_period(hour: int) -> str:\n",
    "    if 6 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"day\"\n",
    "    elif 18 <= hour < 24:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "\n",
    "df[\"STD_hour\"] = df[\"STD\"].dt.hour\n",
    "df[\"STD_period\"] = df[\"STD_hour\"].apply(map_hour_to_period)\n",
    "\n",
    "df[\"flight_time\"] = (df[\"STA\"] - df[\"STD\"]).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the years covered by the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATOP_years = df[\"DATOP_year\"].unique()\n",
    "DATOP_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the data originate from the years 2016, 2017, and 2018.\n",
    "\n",
    "Next, let us consider the distribution of recorded flights across the months within the period from 2016 to 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=len(DATOP_years), figsize=(16, 5), sharey=True)\n",
    "\n",
    "for idx, year in enumerate(DATOP_years):\n",
    "    # Filter the DataFrame for the specific year\n",
    "    df_year = df[df[\"DATOP_year\"] == year]\n",
    "\n",
    "    # Plot the histogram on the respective subplot\n",
    "    axes[idx].hist(df_year[\"DATOP_month\"], bins=range(1, 14), alpha=0.8, color=\"blue\")\n",
    "    axes[idx].set_title(f\"Flight Distribution for {year}\")\n",
    "    axes[idx].set_xlabel(\"Month\")\n",
    "    # Set x-axis ticks for months\n",
    "    axes[idx].set_xticks(range(1, 13))\n",
    "    axes[idx].set_ylabel(\"Number of Flights\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt_savefig(\"month-to-sum-flight-by-year_hist\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each year, we observe a single suspicious month during which the sum of flights is significantly lower than in the others. An (even manual) inspection of the provided test data set from zindi reveals that the majority of flights for the affected months are included therein (sic!). Consequently, we exclude these months entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df[\"DATOP_month\"] == 5) & (df[\"DATOP_year\"] == 2016))]\n",
    "df = df[~((df[\"DATOP_month\"] == 2) & (df[\"DATOP_year\"] == 2017))]\n",
    "df = df[~((df[\"DATOP_month\"] == 9) & (df[\"DATOP_year\"] == 2018))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let us inspect the distribution of delay durations conditioned on to the weekday (and the year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_years = len(DATOP_years)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, year in enumerate(DATOP_years):\n",
    "    ax = axes[i]\n",
    "    df_year = df[df[\"DATOP_year\"] == year]\n",
    "    daily_avg = df_year.groupby(\"DATOP_day\")[\"target\"].mean().reset_index()\n",
    "\n",
    "    ax.bar(daily_avg[\"DATOP_day\"], daily_avg[\"target\"], color=\"blue\", alpha=0.7)\n",
    "\n",
    "    ax.set_title(f\"Average Delay by Day of Week for {year}\", fontsize=12)\n",
    "    ax.set_xlabel(\"Day\", fontsize=10)\n",
    "    ax.set_ylabel(\"Delay\", fontsize=10)\n",
    "    ax.set_xticks(range(1, 8))\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt_savefig(\"month-to-avg-delay-by-year_hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Overview of the Polished Data\n",
    "\n",
    "A random sample from the processed data is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a plot illustrating all uni- and bivariate distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "\n",
    "plt_savefig(\"each-vs-each-wrt-distribution_scatterplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us redraw the joint distribution of flight and delay duration in a dedicated plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df[\"flight_time\"], df[\"target\"], color=\"blue\")\n",
    "plt.xlabel(\"Flight Duration\")\n",
    "plt.ylabel(\"Delay Duration\")\n",
    "plt.xlim(1, 3000)\n",
    "plt.ylim(1, 3000)\n",
    "plt.title(\"Flight Duration vs. Delay Duration\")\n",
    "plt_savefig(\"flight-to-delay_scatterplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise the correlations between the numerical features of our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    ")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "plt_savefig(\"each-vs-each-wrt-correlation_heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data contain numerous categorical features, it is also advisable to compute the predictive power score matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    col\n",
    "    for col in df.columns\n",
    "    if df[col].nunique() > 1 and not col.startswith(\"DATOP_\") and col != \"ID\"\n",
    "]\n",
    "df_tmp = df[cols]\n",
    "\n",
    "pp_scores = pps.matrix(df_tmp)[[\"x\", \"y\", \"ppscore\"]].pivot(\n",
    "    columns=\"x\", index=\"y\", values=\"ppscore\"\n",
    ")\n",
    "\n",
    "pp_scores = pp_scores.round(2)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    pp_scores,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    # cmap=\"Reds\",\n",
    "    linewidths=0.5,\n",
    "    annot=True,\n",
    ")\n",
    "\n",
    "plt_savefig(\"each-vs-each-wrt-pp-score_heatmap\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reuse in subsequent notebooks, we store our processed data frame as a pickle file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(CFG[\"PROCESSED_DATA_PATH\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
