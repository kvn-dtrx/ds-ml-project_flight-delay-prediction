{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Exploratory Analysis\n",
    "\n",
    "## Setup\n",
    "\n",
    "For our analysis, we will need the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extension of Python package pycountry providing conversion functions\n",
    "import pycountry_convert as pc\n",
    "\n",
    "# Python implementation of the Predictive Power Score (PPS)\n",
    "import ppscore as pps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from local_nbutils import CFG, plt_savefig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the data frame containing the flight delay durations (which must be downloaded manually, compare with the previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG[\"TRAIN_DATA_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Inspection\n",
    "\n",
    "For a first inspection, let us run the `info` and the `sample` method of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has no missing values. The meaning behind the column names can be looked up on <https://zindi.africa/competitions/flight-delay-prediction-challenge/data>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| ID | Unique identifier for the flight |\n",
    "| DATOP | Date of flight |\n",
    "| FLTID | Flight number |\n",
    "| DEPSTN | Departure point (station/airport) |\n",
    "| ARRSTN | Arrival point (station/airport) |\n",
    "| STD | Scheduled Time of Departure |\n",
    "| STA | Scheduled Time of Arrival |\n",
    "| STATUS | Flight status (e.g., delayed, canceled) |\n",
    "| AC | Aircraft code |\n",
    "| target | Flight delay (in minutes) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing our dataset with the information given on this webpage, we see that there are more features specified than we can find in our data frame, more precisely, the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| ETD | Expected Time departure |\n",
    "| ETA | Expected Time arrival |\n",
    "| ATD | Actual Time of Departure |\n",
    "| ATA | Actual Time of arrival |\n",
    "| DELAY1 | Delay code 1 |\n",
    "| DUR1 | Delay time 1 |\n",
    "| DELAY2 | Delay code 2 |\n",
    "| DUR2 | Delay time 2 |\n",
    "| DELAY3 | Delay code 3 |\n",
    "| DUR3 | Delay time 3 |\n",
    "| DELAY4 | Delay code 4 |\n",
    "| DUR4 | Delay time 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Inspection\n",
    "\n",
    "????\n",
    "\n",
    "### Status Column\n",
    "\n",
    "We start by analysing the STATUS feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorry for \"statuses\" ...\n",
    "statuses = df[\"STATUS\"].unique()\n",
    "\n",
    "print(\"All Statuses:\")\n",
    "for status in statuses:\n",
    "    print(f\"  {status}\")\n",
    "    print(f\"    Number of entries : {df[df['STATUS'] == status].shape[0]}\")\n",
    "    print(f\"    Mean              : {df[df['STATUS'] == status]['target'].mean()}\")\n",
    "    print(f\"    Median            : {df[df['STATUS'] == status]['target'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table explains the arising status codes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code | Name | Description |\n",
    "| --- | --- | --- |\n",
    "| ATA | Actual Time Arrival| Flights that successfully landed at their destination |\n",
    "| DEP  | Departed | Flights that departed but may not have completed their journey |\n",
    "| RTR  | Returned | Flights that took off but returned to the departure airport due to issues |\n",
    "| SCH  | Scheduled | Flights listed in the schedule, no delay data applicable |\n",
    "| DEL  | Cancelled | Flights that were canceled, treated as permanent delays |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise how delay durations are distributed when we condition on STATUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, status in enumerate(statuses):\n",
    "    ax = axes[idx]\n",
    "    df[df[\"STATUS\"] == status][\"target\"].hist(\n",
    "        bins=50,\n",
    "        log=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(status)\n",
    "    ax.set_xlabel(\"Delay\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt_savefig(\"delay-to-sum-flight-histograms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of DEP (\"Flights that departed but may not have completed their journey\") remains a bit obscure. As only a minority of flights is labelled as DEP, we decide to drop the corresponding data points.\n",
    "\n",
    "Further, it is hard to measure the delay of a DEL (cancelled) flight. A possibility for regular flights would be to take the duration between the DEL flight and the next flight that indeed arrives plus the delay of that flight. But with the same argument as for DEP, we decide to delete rows from the data frame that have DEL as STATUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"STATUS\"].isin([\"DEP\", \"DEL\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Columns\n",
    "\n",
    "We introduce columns reducing the airports of departure and destination to its country, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv(CFG[\"AIRPORTS_DATA_PATH\"])\n",
    "airports = airports[[\"iata_code\", \"iso_country\"]]\n",
    "airports = airports.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bkp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Make this cell idempotent\n",
    "\n",
    "df = df_bkp.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    left_on=\"DEPSTN\",\n",
    "    right_on=\"iata_code\",\n",
    "    how=\"left\",\n",
    ")\n",
    "df.drop(columns=\"iata_code\", inplace=True)\n",
    "df.rename(columns={\"iso_country\": \"country_dep\"}, inplace=True)\n",
    "\n",
    "df = df.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    left_on=\"ARRSTN\",\n",
    "    right_on=\"iata_code\",\n",
    "    how=\"left\",\n",
    ")\n",
    "df.drop(columns=\"iata_code\", inplace=True)\n",
    "df.rename(columns={\"iso_country\": \"country_arr\"}, inplace=True)\n",
    "\n",
    "df[\"country_arr\"].shape\n",
    "\n",
    "df.loc[df[\"DEPSTN\"] == \"SXF\", \"country_dep\"] = \"DE\"\n",
    "df.loc[df[\"ARRSTN\"] == \"SXF\", \"country_arr\"] = \"DE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For converting the iso codes to continent codes, we use the functionality provided by the module `pycountry_convert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_to_continent(iso: str) -> None | str:\n",
    "    try:\n",
    "        continent_code = pc.country_alpha2_to_continent_code(iso)\n",
    "        return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"continent_dep\"] = df[\"country_dep\"].apply(iso_to_continent)\n",
    "df[\"continent_arr\"] = df[\"country_arr\"].apply(iso_to_continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-related Columns\n",
    "\n",
    "The data set contains several columns with date semantics. Let us convert them to the appropriate dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DATOP\"] = pd.to_datetime(df[\"DATOP\"], format=\"%Y-%m-%d\")\n",
    "df[\"STD\"] = pd.to_datetime(df[\"STD\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"STA\"] = pd.to_datetime(df[\"STA\"], format=\"%Y-%m-%d %H.%M.%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can introduce a bunch of further useful date- and time-related features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DATOP_year\"] = df[\"DATOP\"].dt.year\n",
    "df[\"DATOP_month\"] = df[\"DATOP\"].dt.month\n",
    "df[\"DATOP_day\"] = df[\"DATOP\"].dt.dayofweek + 1\n",
    "\n",
    "def map_hour_to_period(hour: int) -> str:\n",
    "    if 6 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"day\"\n",
    "    elif 18 <= hour < 24:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "\n",
    "df[\"STD_hour\"] = df[\"STD\"].dt.hour\n",
    "df[\"STD_period\"] = df[\"STD_hour\"].apply(map_hour_to_period)\n",
    "\n",
    "df[\"flight_time\"] = (df[\"STA\"] - df[\"STD\"]).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see which years the dataset covers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATOP_years = df[\"DATOP_year\"].unique()\n",
    "DATOP_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data originate from the years 2016, 2017, 2018.\n",
    "\n",
    "Next, let us look how the recorded flights are distributed among the months in the time period from 2016 to 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=len(DATOP_years), figsize=(16, 5), sharey=True)\n",
    "\n",
    "for idx, year in enumerate(DATOP_years):\n",
    "    # Filter the DataFrame for the specific year\n",
    "    df_year = df[df[\"DATOP_year\"] == year]\n",
    "    \n",
    "    # Plot the histogram on the respective subplot\n",
    "    axes[idx].hist(df_year[\"DATOP_month\"], bins=range(1, 14), alpha=0.8, color=\"blue\")\n",
    "    axes[idx].set_title(f\"Flight Distribution for {year}\")\n",
    "    axes[idx].set_xlabel(\"Month\")\n",
    "    # Set x-axis ticks for months\n",
    "    axes[idx].set_xticks(range(1, 13))  \n",
    "    axes[idx].set_ylabel(\"Number of Flights\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt_savefig(\"month-to-sum-flight-by-year_hist.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each year, we find a suspicious months in which the sum of flights is significantly less than in the others. Inspecting (even manually) the provided test data set from zindi one sees that the majority of flights for the affected months can be found there (sic!). We drop these months completely: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df[\"DATOP_month\"] == 5) & (df[\"DATOP_year\"] == 2016))]\n",
    "df = df[~((df[\"DATOP_month\"] == 2) & (df[\"DATOP_year\"] == 2017))]\n",
    "df = df[~((df[\"DATOP_month\"] == 9) & (df[\"DATOP_year\"] == 2018))]\n",
    "\n",
    "df = df[df[\"DEPSTN\"] != df[\"ARRSTN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df[\"flight_time\"], df[\"target\"], color=\"blue\")\n",
    "plt.xlabel(\"Flight Time\")\n",
    "plt.ylabel(\"Delay\")\n",
    "plt.xlim(1,3000)\n",
    "plt.ylim(1,3000)\n",
    "plt.title(\"Scatter Plot of X Column vs Y Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_years = len(DATOP_years)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, year in enumerate(DATOP_years):\n",
    "    ax = axes[i]  # Access subplot\n",
    "    df_year = df[df[\"DATOP_year\"] == year]\n",
    "    daily_avg = df_year.groupby(\"DATOP_day\")[\"target\"].mean().reset_index()\n",
    "    \n",
    "    # Plot bar plot on the current subplot\n",
    "    ax.bar(daily_avg[\"DATOP_day\"], daily_avg[\"target\"], color=\"blue\", alpha=0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(f\"Average Delay by Day of Week for {year}\", fontsize=12)\n",
    "    ax.set_xlabel(\"Month\", fontsize=10)\n",
    "    ax.set_ylabel(\"Delay\", fontsize=10)\n",
    "    ax.set_xticks(range(1, 8)) \n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "plt_savefig(\"month-to-avg-delay-by-year_hist\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Look at the Brushed Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random sample from the processed data now looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a plot comprising all single and pairwise distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "\n",
    "plt_savefig(\"each-vs-each-wrt-distribution_scatterplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise the correlation between the numerical features of our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    ")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "plt_savefig(\"each-vs-each-wrt-correlation_heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data contain so many categorical features, it makes also sense to compute the power predictive score matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    col for col in df.columns\n",
    "    if df[col].nunique() > 1 and not col.startswith(\"DATOP_\") and col != \"ID\"\n",
    "]\n",
    "df_tmp = df[cols]\n",
    "\n",
    "pp_scores = pps.matrix(df_tmp)[[\"x\", \"y\", \"ppscore\"]].pivot(\n",
    "    columns=\"x\", index=\"y\", values=\"ppscore\"\n",
    ")\n",
    "\n",
    "pp_scores = pp_scores.round(2)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    pp_scores,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    # cmap=\"Reds\",\n",
    "    linewidths=0.5,\n",
    "    annot=True,\n",
    ")\n",
    "\n",
    "plt_savefig(\"each-vs-each-wrt-pp-score_heatmap\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a reuse in the next notebooks, we store our processed data frame as pickle file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(CFG[\"PROCESSED_DATA_PATH\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
